{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarceloClaro/Iniciador-de-Interface-Web-para-Gera-o-de-Texto/blob/main/Iniciador_de_Interface_Web_para_Gera%C3%A7%C3%A3o_de_Texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"Iniciador de Interface Web para Geração de Texto\"**"
      ],
      "metadata": {
        "id": "290umXStp0JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O projeto descrito envolve a criação de uma interface de usuário web usando a biblioteca Gradio para interagir com Modelos de Linguagem de Grande Escala. Este projeto tem um potencial significativo para estudos e aplicações em diversas áreas. Vamos explorar cada componente e seu potencial para estudo:\n",
        "\n",
        "1. **Gradio web UI:**\n",
        "   - **Potencial para Estudo:** A Gradio permite a rápida prototipagem e demonstração de modelos de machine learning, tornando-a ideal para fins educativos, experimentação e apresentação de projetos de pesquisa. Estudantes e pesquisadores podem usar a Gradio para visualizar como os modelos de PLN funcionam na prática, interagindo com eles em tempo real.\n",
        "\n",
        "2. **Large Language Models:**\n",
        "   - **Potencial para Estudo:** Estes modelos são fundamentais no campo do PLN e oferecem uma rica área de estudo. Eles podem ser explorados para entender a geração de linguagem, análise de sentimentos, respostas a perguntas, tradução automática, e mais. Há um vasto campo para pesquisa em melhorias de algoritmos, eficiência computacional, e aplicações práticas destes modelos.\n",
        "\n",
        "3. **Supports transformers:**\n",
        "   - **Potencial para Estudo:** A arquitetura de transformadores revolucionou o PLN, proporcionando avanços significativos em tarefas como compreensão de texto e geração de linguagem. O estudo destes modelos pode abranger desde a compreensão teórica de sua mecânica até a aplicação em problemas do mundo real, como processamento de linguagem em redes sociais ou sistemas de recomendação.\n",
        "\n",
        "4. **GPTQ, AWQ, EXL2:**\n",
        "   - **Potencial para Estudo:** Estas variantes específicas de modelos de linguagem oferecem oportunidades para explorar diferentes abordagens em PLN. Isso pode incluir estudos sobre eficiência de modelos, capacidade de geração de texto, e aplicações em contextos específicos como chatbots ou sistemas de análise de texto.\n",
        "\n",
        "5. **llama.cpp (GGUF), Llama models:**\n",
        "   - **Potencial para Estudo:** Estudar estas implementações específicas pode fornecer insights sobre como diferentes configurações e otimizações afetam o desempenho dos modelos de PLN. Isso pode levar a inovações em como os modelos são construídos e treinados, bem como a aplicações mais eficientes em termos de recursos.\n",
        "\n",
        "**Conclusão:**\n",
        "O projeto apresenta uma oportunidade excepcional para estudar e aplicar a teoria e prática dos modelos de linguagem de grande escala. A interface Gradio torna os modelos acessíveis e interativos, permitindo aos usuários não apenas estudar os fundamentos teóricos do PLN, mas também experimentar diretamente o impacto e as capacidades desses modelos avançados. Isso pode ser particularmente valioso em ambientes educacionais, de pesquisa, e para desenvolvedores buscando aplicar PLN em soluções práticas."
      ],
      "metadata": {
        "id": "sfUhMY6Su3WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# oobabooga/text-generation-webui\n",
        "\n",
        "Após executar ambas as células, um URL público do gradio aparecerá na parte inferior em alguns minutos. Você pode opcionalmente gerar um link de API.\n",
        "\n",
        "* Página do projeto: https://github.com/oobabooga/text-generation-webui\n",
        "* Status do servidor gradio: https://status.gradio.app/\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Mantenha esta guia ativa para evitar que o Colab desconecte você { display-mode: \"form\" }\n",
        "\n",
        "# Esta linha é um título em uma célula do Colab.\n",
        "\n",
        "# \"{ display-mode: \"form\" }\" é uma configuração que altera a forma como a célula é exibida, tornando-a mais parecida com um formulário.\n",
        "\n",
        "#@markdown Pressione play no music player que aparecerá abaixo:\n",
        "\n",
        "# Esta é uma anotação Markdown fornecendo instruções ao usuário.\n",
        "\n",
        "\n",
        "%%html\n",
        "\n",
        "\n",
        "\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n",
        "\n",
        "# Esta é uma tag HTML para incorporar um player de áudio na célula.\n",
        "# O atributo 'src' especifica o URL do arquivo de áudio a ser reproduzido, que neste caso é \"https://oobabooga.github.io/silence.m4a\".\n",
        "# O atributo 'controls' adiciona controles de reprodução padrão ao player de áudio (como play, pause, volume).\n"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **As listas com as descrições, funções, vantagens e desvantagens para cada alternativa:**\n",
        "\n",
        "### 1. Model URL (`model_url`)\n",
        "**Alternativa Atual:**\n",
        "1. **[TheBloke/MythoMax-L2-13B-GPTQ](https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ)**\n",
        "   - **Descrição:** Modelo de linguagem GPTQ com 13 bilhões de parâmetros.\n",
        "   - **Vantagens:** Capacidade avançada de geração e compreensão de texto.\n",
        "   - **Desvantagens:** Exige alto poder computacional para processamento.\n",
        "\n",
        "**Expansão da Lista:**\n",
        "2. **[openai/gpt-3](https://huggingface.co/openai/gpt-3)**\n",
        "   - **Descrição:** Modelo GPT-3 da OpenAI para diversas tarefas de PLN.\n",
        "   - **Vantagens:** Muito versátil e poderoso em geração de texto.\n",
        "   - **Desvantagens:** Acesso restrito e pode ser caro para uso intensivo.\n",
        "\n",
        "3. **[google/t5-xxl](https://huggingface.co/google/t5-xxl)**\n",
        "   - **Descrição:** Modelo T5 para tarefas de tradução, sumarização e mais.\n",
        "   - **Vantagens:** Flexível em várias tarefas de PLN.\n",
        "   - **Desvantagens:** Requer ajustes específicos para cada tarefa.\n",
        "\n",
        "4. **[facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)**\n",
        "   - **Descrição:** Modelo BART otimizado para sumarização.\n",
        "   - **Vantagens:** Excelente para sumarização de textos.\n",
        "   - **Desvantagens:** Menos versátil para outras tarefas de PLN.\n",
        "\n",
        "5. **[deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2)**\n",
        "   - **Descrição:** RoBERTa treinado para QA (Question Answering) com SQuAD 2.0.\n",
        "   - **Vantagens:** Especializado em responder perguntas com precisão.\n",
        "   - **Desvantagens:** Focado em QA, não tão bom em geração de texto livre.\n",
        "\n",
        "### 2. Branch (`branch`)\n",
        "**Alternativa Atual:**\n",
        "1. **gptq-4bit-32g-actorder_True**\n",
        "   - **Descrição:** Configuração específica do modelo GPTQ.\n",
        "   - **Vantagens:** Otimizado para eficiência em hardware específico.\n",
        "   - **Desvantagens:** Menos flexível para outras configurações de hardware.\n",
        "\n",
        "**Expansão da Lista:**\n",
        "- Dado que as alternativas de `branch` são altamente específicas ao repositório e ao modelo em uso, não é possível listar outras sem conhecer o contexto específico do repositório de modelos.\n",
        "\n",
        "### 3. Command Line Flags (`command_line_flags`)\n",
        "**Alternativa Atual:**\n",
        "1. **--n-gpu-layers 128 --load-in-4bit --use_double_quant**\n",
        "   - **Descrição:** Configuração para 128 camadas de GPU, carregamento em 4 bits, e quantização dupla.\n",
        "   - **Vantagens:** Otimiza o uso da GPU e da memória.\n",
        "   - **Desvantagens:** Pode haver comprometimento na precisão.\n",
        "\n",
        "**Expansão da Lista:**\n",
        "2. **--n-gpu-layers 64 --load-in-8bit**\n",
        "   - **Descrição:** Usa 64 camadas de GPU, carregando em 8 bits.\n",
        "   - **Vantagens:** Menor uso de recursos comparado a 128 camadas.\n",
        "   - **Desvantagens:** Menos capacidade de processamento paralelo.\n",
        "\n",
        "3. **--n-gpu-layers 256 --use_float**\n",
        "   - **Descrição:** Configuração para 256 camadas de GPU, usando precisão de ponto flutuante.\n",
        "   - **Vantagens:** Maximiza a capacidade de processamento e precisão.\n",
        "   - **Desvantagens:** Uso intensivo de recursos, necessitando de hardware robusto.\n",
        "\n",
        "4. **--n-gpu-layers 32 --use_mixed_precision**\n",
        "   - **Descrição:** Usa 32 camadas de GPU com precisão mista.\n",
        "   - **Vantagens:** Bom equilíbrio entre desempenho e uso\n",
        "\n",
        " de recursos.\n",
        "   - **Desvantagens:** Compromisso entre precisão e eficiência.\n",
        "\n",
        "5. **--no-quantization**\n",
        "   - **Descrição:** Desativa a quantização dos pesos do modelo.\n",
        "   - **Vantagens:** Mantém a máxima qualidade e precisão do modelo.\n",
        "   - **Desvantagens:** Aumenta significativamente o uso de memória.\n",
        "\n",
        "Essas alternativas oferecem um leque de opções que podem ser ajustadas de acordo com as necessidades específicas da tarefa, recursos disponíveis e objetivos do projeto."
      ],
      "metadata": {
        "id": "5Z8digqkx8nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vou explicar cada item do código minuciosamente, comentando o código existente:\n",
        "\n",
        "```python\n",
        "# Importação das bibliotecas necessárias.\n",
        "import torch\n",
        "from pathlib import Path\n",
        "```\n",
        "Essas linhas importam as bibliotecas necessárias para o código. `torch` é a biblioteca PyTorch, amplamente utilizada em aprendizado de máquina, e `Path` é uma classe para trabalhar com caminhos de arquivo.\n",
        "\n",
        "```python\n",
        "# Verifica se o diretório atual é 'text-generation-webui'. Se não for, instala a web UI.\n",
        "if Path.cwd().name != 'text-generation-webui':\n",
        "  print(\"Installing the webui...\")\n",
        "  # Clona o repositório do GitHub que contém o código da web UI.\n",
        "  !git clone https://github.com/oobabooga/text-generation-webui\n",
        "  %cd text-generation-webui\n",
        "```\n",
        "Este trecho verifica se o código está sendo executado no diretório 'text-generation-webui'. Se não estiver, ele clona um repositório GitHub que contém o código da interface de usuário web para geração de texto. Em seguida, ele muda o diretório de trabalho para 'text-generation-webui'.\n",
        "\n",
        "```python\n",
        "  # Verifica a versão do PyTorch instalada.\n",
        "  torver = torch.__version__\n",
        "  print(f\"TORCH: {torver}\")\n",
        "  is_cuda118 = '+cu118' in torver  # Verifica se é a versão com CUDA 11.8.\n",
        "  is_cuda117 = '+cu117' in torver  # Verifica se é a versão com CUDA 11.7.\n",
        "```\n",
        "Esta parte verifica a versão do PyTorch instalada e armazena em `torver`. Em seguida, ele verifica se a versão do PyTorch inclui 'cu118' ou 'cu117', indicando se é uma versão com suporte a CUDA 11.8 ou CUDA 11.7.\n",
        "\n",
        "```python\n",
        "  # Lê as dependências necessárias do arquivo 'requirements.txt'.\n",
        "  textgen_requirements = open('requirements.txt').read().splitlines()\n",
        "```\n",
        "Aqui, ele lê as dependências necessárias do arquivo 'requirements.txt' e armazena em `textgen_requirements`.\n",
        "\n",
        "```python\n",
        "  # Ajusta as dependências com base na versão do CUDA.\n",
        "  if is_cuda117:\n",
        "      textgen_requirements = [req.replace('+cu121', '+cu117').replace('+cu122', '+cu117').replace('torch2.1', 'torch2.0') for req in textgen_requirements]\n",
        "  elif is_cuda118:\n",
        "      textgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
        "  with open('temp_requirements.txt', 'w') as file:\n",
        "      file.write('\\n'.join(textgen_requirements))\n",
        "```\n",
        "Nesta seção, dependendo da versão do CUDA encontrada anteriormente, ele faz ajustes nas dependências listadas em `textgen_requirements`. Por exemplo, substituindo '+cu121' por '+cu117' ou '+cu118', e 'torch2.1' por 'torch2.0'. Em seguida, ele cria um arquivo 'temp_requirements.txt' atualizado com as dependências ajustadas.\n",
        "\n",
        "```python\n",
        "  # Instala as dependências.\n",
        "  !pip install -r extensions/openai/requirements.txt --upgrade\n",
        "  !pip install -r temp_requirements.txt --upgrade\n",
        "```\n",
        "Aqui, ele usa o `pip` para instalar as dependências listadas em 'extensions/openai/requirements.txt' e 'temp_requirements.txt', ambas atualizadas com as modificações necessárias.\n",
        "\n",
        "```python\n",
        "  # Exibe mensagens sobre a instalação.\n",
        "  print(\"\\033[1;32;1m\\n --> If you see a warning about \\\"previously imported packages\\\", just ignore it.\\033[0;37;0m\")\n",
        "  print(\"\\033[1;32;1m\\n --> There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "```\n",
        "Essas linhas exibem mensagens informativas sobre a instalação das dependências, indicando como lidar com possíveis avisos e esclarecendo que não é necessário reiniciar o ambiente de execução.\n",
        "\n",
        "```python\n",
        "  # Tenta importar 'flash_attn', se falhar, desinstala.\n",
        "  try:\n",
        "    import flash_attn\n",
        "  except:\n",
        "    !pip uninstall -y flash_attn\n",
        "```\n",
        "Aqui, ele tenta importar o módulo 'flash_attn'. Se a importação falhar, ele desinstala o pacote 'flash_attn' usando o comando `pip uninstall -y flash_attn`.\n",
        "\n",
        "```python\n",
        "# Define os parâmetros para o modelo.\n",
        "model_url = \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ\" #@param {type:\"string\"}\n",
        "branch = \"gptq-4bit-32g-actorder_True\" #@param {type:\"string\"}\n",
        "command_line_flags = \"--n-gpu-layers 128 --load-in-4bit --use_double_quant\" #@param {type:\"string\"}\n",
        "api = True #@param {type:\"boolean\"}\n",
        "```\n",
        "Nesta parte, são definidos os parâmetros para o modelo. O `model_url` especifica a URL do modelo a ser usado. A `branch` define a branch do repositório Git a ser usada. `command_line_flags` são as opções de linha de comando que serão passadas ao iniciar a interface de usuário web. `api` é uma variável booleana que indica se a interface de usuário web deve ser executada em modo API.\n",
        "\n",
        "```python\n",
        "# Adiciona parâmetros para a API, se necessário.\n",
        "if api:\n",
        "  for param in ['--api', '--public-api']:\n",
        "    if param not in command_line_flags:\n",
        "      command_line_flags += f\" {param}\"\n",
        "```\n",
        "Se `api` for verdadeiro, ele adiciona os parâmetros `--api` e `--public-api` a `command_line_flags` se eles ainda não estiverem presentes.\n",
        "\n",
        "```python\n",
        "# Trata o URL do modelo.\n",
        "model_url = model_url.strip()\n",
        "if model_url != \"\":\n",
        "    if not model_url.startswith('http'):\n",
        "        model_url = 'https://huggingface.co/' + model_url\n",
        "\n",
        "    # Baixa o modelo.\n",
        "    url_parts = model_url.strip('/').strip().split('/')\n",
        "    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
        "    branch = branch.strip('\"\\' ')\n",
        "    if branch.strip() not in ['', 'main']:\n",
        "        output_folder += f\"_{branch}\"\n",
        "        !python download-model.py {model_url} --branch {branch}\n",
        "    else:\n",
        "        !python download-model.py {model_url}\n",
        "else:\n",
        "    output_folder = \"\"\n",
        "```\n",
        "Nesta seção, o código trata o URL do modelo. Ele verifica se o URL é uma URL completa (começando com 'http'). Se não for, adiciona o prefixo correto. Em seguida, ele divide o URL para obter\n",
        "\n",
        " partes relevantes e cria um nome de pasta (`output_folder`) com base nas partes do URL. Se a branch não estiver vazia ou igual a 'main', ela é incluída no nome da pasta. Em seguida, ele usa um script chamado 'download-model.py' para baixar o modelo com o URL fornecido.\n",
        "\n",
        "```python\n",
        "# Inicia a interface de usuário web.\n",
        "cmd = f\"python server.py --share\"\n",
        "if output_folder != \"\":\n",
        "    cmd += f\" --model {output_folder}\"\n",
        "cmd += f\" {command_line_flags}\"\n",
        "print(cmd)\n",
        "!$cmd\n",
        "```\n",
        "Por fim, esta parte do código inicia a interface de usuário web. Ele monta um comando de linha de acordo com as configurações e parâmetros definidos anteriormente. O comando é então executado usando `!$cmd`, iniciando a interface de usuário web para o modelo de geração de texto."
      ],
      "metadata": {
        "id": "FpFNEjKs47tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.  Iniciar a interface de usuário web\n",
        "\n",
        "# Esta linha é um título em uma célula do Jupyter Notebook, usado para descrever visualmente o que a célula faz.\n",
        "# Neste caso, indica que esta célula é responsável por lançar a interface de usuário web (web UI).\n",
        "\n",
        "#@markdown  Se não tiver certeza sobre a branch, escreva \"main\" ou deixe em branco.\n",
        "\n",
        "# Esta linha é uma anotação em formato Markdown, que fornece uma instrução ao usuário.\n",
        "# Sugere que, se o usuário estiver incerto sobre qual branch do repositório Git usar, deve escrever \"main\" ou deixar em branco.\n",
        "\n",
        "\n",
        "# Importação das bibliotecas necessárias.\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Verifica se o diretório atual é 'text-generation-webui'. Se não for, instala a web UI.\n",
        "if Path.cwd().name != 'text-generation-webui':\n",
        "  print(\"Installing the webui...\")\n",
        "\n",
        "  # Clona o repositório do GitHub que contém o código da web UI.\n",
        "  !git clone https://github.com/oobabooga/text-generation-webui\n",
        "  %cd text-generation-webui\n",
        "\n",
        "  # Verifica a versão do PyTorch instalada.\n",
        "  torver = torch.__version__\n",
        "  print(f\"TORCH: {torver}\")\n",
        "  is_cuda118 = '+cu118' in torver  # Verifica se é a versão com CUDA 11.8.\n",
        "  is_cuda117 = '+cu117' in torver  # Verifica se é a versão com CUDA 11.7.\n",
        "\n",
        "  # Lê as dependências necessárias do arquivo 'requirements.txt'.\n",
        "  textgen_requirements = open('requirements.txt').read().splitlines()\n",
        "  # Ajusta as dependências com base na versão do CUDA.\n",
        "  if is_cuda117:\n",
        "      textgen_requirements = [req.replace('+cu121', '+cu117').replace('+cu122', '+cu117').replace('torch2.1', 'torch2.0') for req in textgen_requirements]\n",
        "  elif is_cuda118:\n",
        "      textgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
        "  with open('temp_requirements.txt', 'w') as file:\n",
        "      file.write('\\n'.join(textgen_requirements))\n",
        "\n",
        "  # Instala as dependências.\n",
        "  !pip install -r extensions/openai/requirements.txt --upgrade\n",
        "  !pip install -r temp_requirements.txt --upgrade\n",
        "\n",
        "  # Exibe mensagens sobre a instalação.\n",
        "  print(\"\\033[1;32;1m\\n --> If you see a warning about \\\"previously imported packages\\\", just ignore it.\\033[0;37;0m\")\n",
        "  print(\"\\033[1;32;1m\\n --> There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "\n",
        "  # Tenta importar 'flash_attn', se falhar, desinstala.\n",
        "  try:\n",
        "    import flash_attn\n",
        "  except:\n",
        "    !pip uninstall -y flash_attn\n",
        "\n",
        "# Define os parâmetros para o modelo.\n",
        "model_url = \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ\" #@param {type:\"string\"}\n",
        "branch = \"gptq-4bit-32g-actorder_True\" #@param {type:\"string\"}\n",
        "command_line_flags = \"--n-gpu-layers 128 --load-in-4bit --use_double_quant\" #@param {type:\"string\"}\n",
        "api = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Adiciona parâmetros para a API, se necessário.\n",
        "if api:\n",
        "  for param in ['--api', '--public-api']:\n",
        "    if param not in command_line_flags:\n",
        "      command_line_flags += f\" {param}\"\n",
        "\n",
        "# Trata o URL do modelo.\n",
        "model_url = model_url.strip()\n",
        "if model_url != \"\":\n",
        "    if not model_url.startswith('http'):\n",
        "        model_url = 'https://huggingface.co/' + model_url\n",
        "\n",
        "    # Baixa o modelo.\n",
        "    url_parts = model_url.strip('/').strip().split('/')\n",
        "    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
        "    branch = branch.strip('\"\\' ')\n",
        "    if branch.strip() not in ['', 'main']:\n",
        "        output_folder += f\"_{branch}\"\n",
        "        !python download-model.py {model_url} --branch {branch}\n",
        "    else:\n",
        "        !python download-model.py {model_url}\n",
        "else:\n",
        "    output_folder = \"\"\n",
        "\n",
        "# Inicia a interface de usuário web.\n",
        "cmd = f\"python server.py --share\"\n",
        "if output_folder != \"\":\n",
        "    cmd += f\" --model {output_folder}\"\n",
        "cmd += f\" {command_line_flags}\"\n",
        "print(cmd)\n",
        "!$cmd\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}